# unbalanced_valid_split
## 1.【问题描述】
    非平衡数据的分类问题，假设我使用过采样解决不平衡问题，并从训练集切分出验证集进行调参，那么可选用的方案：<br>
（0）.不进行过采样，将原始训练集切分成train-train,train-dev,在train-train上训练，在train-dev上调参;<br>
（1）.从原始训练集分出来train-dev（train-dev的标签比例1：1），对剩余的训练集数据进行过采样train-train，再训练;<br>
（2）.从原始训练集分层抽样，抽出train-dev（train-dev同原始fei'ping'heng'shi标签比例），对剩余的训练集数据进行过采样train-train，再训练;<br>
（3）.对原始的训练集数据进行过采样，从过采样后的训练集切分train-train,train-dev，再训练;<br>
<br>
按直觉来分析：<br>
方案3，不行，这样验证集和训练集可能有重复，会过拟合；<br>
方案2，由于验证集是非平衡的，调参又会让模型整个又倾向于非平衡时的学习能力，模型就又退步了；<br>

如果上述判断没错的话，【方案1】应该是实际操作时的合适选择。<br>
<br>
## 2.【实验分析】
我简单做了实验, 对一个不平衡数据(12953:1365)实现逻辑回归分类模型,先切分80%train和20%test。<br>
Train:【对train的切分和过采样方式，按照4种方案进行】<br>
    train-train: 训练模型的数据集<br>
    train-dev：留出法调参的验证集 (4种方案的验证集数据量均相同,但因为数据不平衡，不敢设太大)<br>
test：<br>
    test：测试数据<br>
实验结果在同一个test上的accuracy表现如下：<br>

![result](https://github.com/badbadcode/unbalanced_valid_split/blob/master/img/result.png)

## 3.【待解决问题】
   三种过采样的切分方案，看似效果差距不大，如果不是数据个例的原因，分析这个结果：<br>
 a. 采取过采样的三种方案，均比【不进行过采样】的效果要差，过采样 使模型对少数类过拟合，所以效果下降？那在实际操作中，有没有经验可以指导什么时候要处理非平衡的问题呢？<br>
 b. 若 a 分析成立，为何方案1（平衡数据下的调参）的泛化能力看起来又比方案2（倾向于数据稀疏化方向的调参）的泛化能力要好一些呢？<br>
 c. 通常实践中的切分方案是哪个呢？
